{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inputdata import InputData\n",
    "from skipgram import SkipGramModel\n",
    "import torch.optim as optim\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, input_file, output_file, emb_dimension=100, batch_size=100, window_size=5, iteration=5,\n",
    "                initial_lr=0.025, min_count=5, context_size=2):\n",
    "    # iteration: 进行几次walks，即几次number_paths*nodes \n",
    "    # min_count: ?\n",
    "    # context_size: ?\n",
    "    # batch_size: ?\n",
    "        self.data = InputData(input_file, min_count)\n",
    "        self.output_file = output_file\n",
    "        self.emb_size = len(self.data.word2id)#？\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.iteration = iteration\n",
    "        self.initial_lr = initial_lr\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension) # ?\n",
    "        self.optimizer = optim.SGD(self.skip_gram_model.parameters(), lr=self.initial_lr) # 参数是什么如何传，如何优化\n",
    "        \n",
    "    def skip_gram_train(self):\n",
    "        pair_count = self.data.evaluate_pair_count(self.window_size)# 输入窗口尺寸就可以得到点对数量（中心点和临近点）\n",
    "        batch_count = self.iteration * pair_count / self.batch_size# 点对数量*迭代次数/batch_size 就是batch的数量\n",
    "        for i in range(int(batch_count)):\n",
    "            # 每次都一样 ？？？\n",
    "            word_pairs = self.data.get_batch_pairs(self.batch_size, self.window_size)# 根据batch_size和window_size取出点对\n",
    "            pos_pairs, neg_pairs = self.data.get_pairs_from_huffman(word_pairs)# huffman采样\n",
    "            \n",
    "            pos_center = [int(pair[0]) for pair in pos_pairs] # 是什么是huffman编码吗\n",
    "            pos_window = [int(pair[1]) for pair in pos_pairs]\n",
    "            neg_center = [int(pair[0]) for pair in neg_pairs]\n",
    "            neg_window = [int(pair[1]) for pair in neg_pairs]\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.skip_gram_model.forward(pos_center, pos_window, neg_center, neg_window)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if i * self.batch_size % 100000 == 0:\n",
    "                lr = self.initial_lr * (1.0 - 1.0 * i / batch_count)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "        self.skip_gram_model.save_embedding(self.data.id2word, self.output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
