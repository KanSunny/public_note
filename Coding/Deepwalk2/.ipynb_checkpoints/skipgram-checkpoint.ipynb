{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable #弃用\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        huffman_node_sum = 2 * emb_size - 1\n",
    "        self.center_embeddings = nn.Embedding(huffman_node_sum, emb_dimension, sparse=True)\n",
    "        self.window_embeddings = nn.Embedding(huffman_node_sum, emb_dimension, sparse=True)\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.center_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.window_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, pos_center, pos_window, neg_center, neg_window):# 是否要注意一下负例的个数??\n",
    "        losses = []\n",
    "        pos_emb_center = self.center_embeddings(Variable(torch.LongTensor(pos_center)))\n",
    "        pos_emb_window = self.window_embeddings(Variable(torch.LongTensor(pos_window)))\n",
    "        pos_score = torch.mul(pos_emb_center, pos_emb_window)\n",
    "        pos_score = torch.sum(pos_score, dim=1)\n",
    "        pos_score = F.logsigmoid(pos_score)\n",
    "        losses.append(sum(pos_score))\n",
    "        neg_emb_center = self.center_embeddings(Variable(torch.LongTensor(neg_center)))\n",
    "        neg_emb_window = self.window_embeddings(Variable(torch.LongTensor(neg_window)))\n",
    "        neg_score = torch.mul(neg_emb_center, neg_emb_window)\n",
    "        neg_score = torch.sum(neg_score, dim=1)\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        losses.append(sum(neg_score))\n",
    "        return -1 * sum(losses)\n",
    "\n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.center_embeddings.weight.data.numpy()\n",
    "        fout = open(file_name, 'w', encoding=\"UTF-8\")\n",
    "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
    "        for wid, word in id2word.items():\n",
    "            emb = embedding[wid]\n",
    "            emb = ' '.join(map(lambda x: str(x), emb))\n",
    "            fout.write('%s %s\\n' % (word, emb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
